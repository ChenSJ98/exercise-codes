\documentclass[conference]{IEEEtran}

  \usepackage{booktabs}
  \usepackage{listing}
  \usepackage{amsmath}
  \usepackage{algorithm}
  \usepackage{array}
  \usepackage{url}
  \usepackage{cite}
  \usepackage{complexity}
\usepackage{algpseudocode}
\usepackage{graphicx}
% \usepackage{algorithm}
  \ifCLASSINFOpdf
  
  \else
  
  \fi
  
  
  \begin{document}
  
  \title{CS303 Project4: Support Vector Machine Using PEGASOS Algorithm}
  
  \author{\IEEEauthorblockN{Shijie Chen 11612028}
  \IEEEauthorblockA{Department of Computer Science and Engineering\\
  Southern University of Science and Technology\\
  Shenzhen, Guangdong, China\\
  Email: 11612028@mail.sustc.edu.cn}
  }
  
  \maketitle
  
  \begin{abstract}
    Support Vector Machine (SVM) is a classic machine learning approach to solve classicification problems. In this project, I implemented a SVM using the PEGASOS algorithm and compared the performance of PEGASOS algorithm with SMO.
  \end{abstract}
  \IEEEpeerreviewmaketitle
  
  \section{Preliminaries}
    \subsection{Notation}
    The following notations are used in this report:
    \begin{table}[H]
        \caption{representation}
        \centering
        \begin{tabular}{cccc}
        \toprule
        Name&Variable\\
        \midrule
        Weight Matrex&$w$\\
        Vector&$x$\\
        Class label&$y$\\
        Cernel Function&$K(x,y)$\\
        Lagrange Multipliers&$\alpha_i$\\
            
        \bottomrule
        \end{tabular}
        \label{table:1}
        \end{table}
    \subsection{Support Vector Machine}
    In machine learning, support vector machines (SVMs) are supervised models associated with classification and regression problems.

    Consider a classification problem given a set of training examples, ${x_1,y_1},\ldots,{x_n,y_n}$, where $x_i$ is an input vector and $y_i \in {-1,1}$ is the correspoding class label. A SVM is trained with training examples and assigns class label $y\in{-1,1}$to a new vector $x$.


    SVMs construct a decision plane, which is a hyperplane $wx+b=0$ to decide the category of a given vector. Since a larger margin, distance of the nearest training pattern to the decision plane, leads to better generalizing ability, the problem of getting a good decision plane can be expressed as the following convex quadratic programming problem:
    \begin{equation*}
    \begin{aligned}
    & \underset{x}{\text{minimize}}
    & & \frac{1}{2}\left|w\right|^2 \\
    & \text{subject to}
    & & y_i((w\cdot x_i)+b)\geq 1, \; i = 1, \ldots, l.
    \end{aligned}
    \end{equation*}
    Alternatively, we can solve its dual problem:
    \begin{equation*}
    \begin{aligned}
    & \underset{x}{\text{maximize}}
    & & \sum_{i = 1}^n{\alpha_i} - \frac{1}{2}\sum_{i = 1}^n\sum_{j=1}^n y_iy_jK(x_i,x_j)\alpha_i\alpha_j \\
    & \text{subject to}
    & & 0\leq\alpha_i\leq C,\sum_{i = 1}^ny_i\alpha_i=0, \; i = 1, \ldots, l.
    \end{aligned}
    \end{equation*}
    where $C$ is an hyperparameter and $K(x_i,y_i)$ is the kernel function, both given by the user. $\alpha_i$ are Lagrange multipliers used to construct the dual problem.

    In this project, such a problem is solved using two methods: Primal Estimated sub-GrAdient
    SOlver for SVM (PEGASOS) and Sequential Minimal Optimization (SMO).

    Performance is measured by the accuracy of the classifier on test dataset and the time needed for training.
 

   

    \section{Methodology}
    \subsection{Data Structure}
    Data structure used in this project is numpy.array.
   \subsection{The PEGASOS Algorithm}
        The PEGASOS algorithm is a kind of gradient descendent method. It performs gradient descent on each training data with a decreasing step size\cite{shalev2011pegasos}.
        

        \begin{algorithm}[H]
        \begin{algorithmic}[1]
            \Function{PEGASOS}{$data,y$}
            \State $w_1 \gets 0$
            \State $t \gets 0$
            \For {$iter = 1,\ldots,20$}
                \For {$j = 1,\ldots,\left| data \right|$}
                    \State $t \gets t+1$
                    \State $\eta_t \gets \frac{1}{t\lambda}$
                    \If {$y_j(w_tx_j<1$}
                    \State $w_{t+1} \gets (1-\eta_t\lambda)w_t + \eta_ty_jx_j$
                    \Else
                    \State $w_{t+1} \gets (1-\eta_t\lambda)w_t$
                    \EndIf
                \EndFor
            \EndFor


            \Return $w_{t+1}$
            \EndFunction
        \end{algorithmic}
        \caption{PEGASOS}
    \end{algorithm}

    \subsection{The SMO algorithm}
    The SMO algorithm\cite{platt1998sequential} breaks the dual problem to its smallest parts. The smallest subproblem is to solve a problem with only two lagrange multipliers $\alpha_1,\alpha_2$ analytically with constraints:
    
    \begin{equation*}
        0\leq\alpha_1\alpha_2\leq C
    \end{equation*}
    \begin{equation*}
        y_1\alpha_1+y_2\alpha_2=k
    \end{equation*}
    where $k$ is the negative of the sum of the rest equality constraints.

    The SMO algorithm works as follows:
    \begin{enumerate}
        \item Find a $\alpha_1$ that violates the Karush-Kuhn-Tucker (KKT) conditions the most.
        \item Find a $\alpha_2$ that optimizes $(\alpha_1,\alpha_2)$
        \item Repeat from 1) until KKT condition is satisfied.
    \end{enumerate}
    
    Since the SMO algorithm checks the KKT conditions, it guarantees an optimal solution if the training set is linearly seperable.

    However, since there're $n(n+1)/2$ combinations of $\alpha_1$ and $\alpha_2$, hueristics are used to cut computation time in popular implementations.
\subsection{Parameters}

The $\lambda$ in PEGASOS is a hyperparameter that controls the learning rate and is given by the user. Through experiments, I found $\lambda = 0.05$ a good choice in this project.

\section{Validation}
\subsection{Environment}     
\begin{itemize}
    \item OS: windows 10
    \item RAM: 16G
    \item CPU: Intel Core i7 8700k @ 3.7GHz
    \item Python: 3.7.1
\end{itemize}

\subsection{Benchmark}
I used the breast cancer dataset to validate my implementation of PEGASOS algorithm. For comparison, I used the SVM.SVC from $scikit-learn$ package whose implementation is based on SMO algorithm. Kernel function of SMO is set as $radial\; basis\; function (rbf)$ and $linear$, 

Note that SVM of $sklearn$ is implementated in C and PEGASOS is implementated in python. Running time is not compared in this report.
\begin{table}[H]
    \caption{representation}
    \centering
    \begin{tabular}{cccccc}
    \toprule
    Benchmark &Dim&Model & $N_{train}$&$N_{test}$&Accuracy \\
    \midrule

    $train\_data$&10&PEGASOS&2000&2000&0.991 \\
    $train\_data$&10&PEGASOS&2000&600&0.990\\
    $train\_data$&10&SMO(rbf)&2000&2000&1.000\\
    $train\_data$&10&SMO(linear)&2000&2000&0.998\\
    $train\_data$&10&SMO(rbf)&2000&600&1.000\\
    $train\_data$&10&SMO(linear)&2000&600&1.000\\
    Breast Cancer&9&PEGASOS&2000&2000&0.737\\
    Breast Cancer&9&PEGASOS&2000&700&0.710\\
    Breast Cancer&9&SMO(rbf)&2000&2000&0.906\\
    Breast Cancer&9&SMO(linear)&2000&2000&0.710\\
    Breast Cancer&9&SMO(rbf)&2000&700&0.867\\
    Breast Cancer&9&SMO(linear)&2000&700&0.736\\
    \bottomrule
    \end{tabular}
    \label{table:1}
    \end{table}

\section{Conclusion}
The PEGASOS algorithm is easy to implement and performs well when the training data is linearly seperable. 

Also note that SMO has better performance even with a linear kernel. A larger leap in accuracy can be get if we can choose the right kernel function, as shown in the Breast Cancer dataset.


\section{Acknowledgement}
The author would like to thank Ying Zhou for providing the preprocessed Breast Cancer dataset.


\bibliographystyle{ieeetr}
\bibliography{ref}
% that's all folks
\end{document}


  